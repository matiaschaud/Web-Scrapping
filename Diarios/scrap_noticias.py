# -*- coding: utf-8 -*-
"""scrap_noticias.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1nx9k23f0xh9WNZZLGVQa-LiY6qXR07Qk
"""

import requests
from bs4 import BeautifulSoup
import re
from selenium import webdriver
# from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
# from selenium.webdriver.support import expected_conditions as EC
import time
from datetime import datetime
# from paquetes.utils import *
import pandas as pd

# Configuramos rutas
DRIVER_DIR = "C:\\chromedriver_win32\\chromedriver.exe"
OUTPUT_DIR = "C:\\Users\\Matias\\Documents\\Matias\\Repositorios\\Web Scrapping\\Diarios\\output"

# Funciones que ayudan a pasar a un formato timestamp en la fecha de la noticia.
def mes_esp_to_eng(txt):
  return txt.replace('enero','jan').replace('febrero','feb').replace('marzo','mar').replace('abril','apr').replace('mayo','may').replace('junio','jun').replace('julio','jul').replace('agosto','aug').replace('septiembre','sep').replace('octubre','oct').replace('noviembre','nov').replace('diciembre','dec')

def mes_esp_to_eng_upper(txt):
  return txt.replace('Enero','jan').replace('Febrero','feb').replace('Marzo','mar').replace('Abril','apr').replace('Mayo','may').replace('Junio','jun').replace('Julio','jul').replace('Agosto','aug').replace('Septiembre','sep').replace('Octubre','oct').replace('Noviembre','nov').replace('Diciembre','dec')

def timpestamp_pag12(fecha):  
    return datetime.strptime(fecha, '%Y-%m-%d')
def timpestamp_clarin(fecha):  
    return datetime.strptime(fecha, '%d/%m/%Y %H:%M')
def timpestamp_lanacion(fecha):  
    return datetime.strptime(fecha, '%d de %b de %Y  •  %H:%M')
def timpestamp_elcronista(fecha):  
    return datetime.strptime(fecha, '%Y-%m-%d %H:%M:%S')
def timpestamp_infobae(fecha):  
    return datetime.strptime(fecha, '%d de %b de %Y')
def timpestamp_ambito(fecha):  
    return datetime.strptime(fecha, '%d %b %Y - %H:%M')

# WARNING!: este código solo sirve en caso de estar ejecutando el scrap desde la pc descargando el código .py
# ya que colab ni jupyter permite abrir popup.
"""
 Sirve para los casos en los que las páginas de secciones de noticias vayan cargando noticias hacia abajo, 
 la función abre un popup no modal para condicionar la página principal de noticias, cargar todas las noticias que se quieran escrapear y
 por último se debe dar clic en el botón "SCRAP!" del popup no modal.
"""

import tkinter as tk
from tkinter import messagebox


class MsgboxScrap():

  def ExitApplication(self):
    self.root.destroy()

  def display_buttom(self):
    self.root= tk.Tk()
    canvas1 = tk.Canvas(self.root, width = 300, height = 100)
    canvas1.pack()
            
    button1 = tk.Button (self.root, text='SCRAP!',command=self.ExitApplication,bg='green',fg='white')
    canvas1.create_window(150, 50, window=button1)
    
    self.root.mainloop()

box_scrap = MsgboxScrap()


"""## Página 12"""

def obtiene_noticias_pag12(pag_base,pag):
  resp = requests.get(pag_base + str(pag))
  
  soup = BeautifulSoup(resp.content, features = "html")
  div_noticias = soup.findAll("div", {"class": "article-item__content"})
  noticias = []
  for div in div_noticias:
    noticias.append(div.find("a").get("href"))
  return noticias

def lee_noticia_pag12(link_noticia):
    print(link_noticia)
    n = {}

    n['link_noticia'] = link_noticia
    resp = requests.get(link_noticia)

    # si la respuesta del servidor es mala, sale de la ejecución de esta noticia.
    if resp.ok == False:
      return None

    soup = BeautifulSoup(resp.content, features = "html")

    # obtiene distintas partes de la noticia:
    articulo_completo = soup.find("div", {"class": "article-inner padding-right"})
    info = articulo_completo.find("div", {"class": "article-info"})
    titulos = articulo_completo.find("div", {"class": "article-titles"})
    body = articulo_completo.find("div", {"class": "article-body diario"})
    imagen = articulo_completo.find("div", {"class": "article-main-media-image"})


    # obtiene atributos

    try:
      raw_fecha = info.find("div",{"class": "time"}).find("span").attrs["datetime"]
      n['fecha'] = timpestamp_pag12(raw_fecha)
    except:
      n['fecha'] = None


    try:
      n['seccion'] = info.find("div", {"class": "suplement"}).text.replace("\r","").replace("\n","").replace("\t","")
    except:
      n['seccion'] = None
    try:
      n['volanta'] = titulos.find("h2",{"class":"article-prefix"}).text.replace("\r","").replace("\n","").replace("\t","")
    except:
      n['volanta'] = None
    try:
      n['titulo'] = titulos.find("h1",{"class":"article-title"}).text.replace("\r","").replace("\n","").replace("\t","")
    except:
      n['titulo'] = None
    try:
      n['resumen'] = titulos.find("div",{"class":"article-summary"}).text.replace("\r","").replace("\n","").replace("\t","")
    except:
      n['resumen'] = None
    try:
      n['epigrafe'] = imagen.find("div",{"class": "article-main-media-text"}).text.replace("\r","").replace("\n","").replace("\t","")
    except:
      n['epigrafe'] = None

    articulo_texto = body.find("div",{"class":"article-text"})
    parrafos = articulo_texto.findAll("p")
    titulos_parrafos = articulo_texto.findAll("h2")

    text_parrafo = ""
    for item in parrafos:
      text_parrafo += "||" + item.text

    text_titulos_parrafos = ""
    for item in titulos_parrafos:
      text_titulos_parrafos += "||" + item.text

    n['titulos_parrafos'] = text_titulos_parrafos.strip().replace("\r","").replace("\n","").replace("\t","")
    n['parrafos'] = text_parrafo.strip().replace("\r","").replace("\n","").replace("\t","")

    return n

def scrap_pag12(desde_pag,hasta_pag):
  l = []
  for pag in range(desde_pag,hasta_pag + 1):
    noticias = obtiene_noticias_pag12("https://www.pagina12.com.ar/secciones/economia?page=",pag)
    for noticia in noticias:
      scrap_not = lee_noticia_pag12(noticia)
      if scrap_not != None:
        l.append(scrap_not)
  return pd.DataFrame(l)


"""## Clarín"""

def obtiene_noticias_clarin(pag, usar_webdriver = False, webdriver_dir = ""):
  
  if usar_webdriver == False:
    resp = requests.get(pag)
    soup = BeautifulSoup(resp.content, features = "html")
  else:
    browser = webdriver.Chrome(webdriver_dir)
    browser.get(pag)
    box_scrap.display_buttom()
    soup = BeautifulSoup(browser.page_source,"html.parser",multi_valued_attributes=None)

  div_noticias = soup.findAll("article")
  noticias = []
  for div in div_noticias:
    try:
      noticias.append("https://www.clarin.com/" + div.find("a").get("href"))
    except:
      pass
  return noticias

def lee_noticia_clarin(link_noticia):
    print(link_noticia)
    n = {}
    n['link_noticia'] = link_noticia
    resp = requests.get(link_noticia)
    
    # si la respuesta del servidor es mala, sale de la ejecución de esta noticia.
    if resp.ok == False:
      return None

    soup = BeautifulSoup(resp.content, features = "html")

    # obtiene distintas partes de la noticia:
    try:
      titulos = soup.find("div", {"class": "title"})
    except:
      return {'link_noticia': link_noticia}
    try:
      imagen_ppal = soup.find("div",{"id": "galeria-trigger"})
    except:
      n['epigrafe'] = None
      imagen_ppal = None

    try:
      body = soup.find("div", {"class": "body-nota"})
    except:
      return {'link_noticia': link_noticia}

    # obtiene atributos

    try:
      raw_fecha = soup.find("span",{"class": "publishedDate"}).text.replace("\r","").replace("\n","").replace("\t","")
      n['fecha'] = timpestamp_clarin(raw_fecha)
    except:
      n['fecha'] = None

    try:
      n['seccion'] = soup.find("li",{"class":"section-name-content"}).text.replace("\r","").replace("\n","").replace("\t","")
    except:
      n['seccion'] = None
    try:
      n['volanta'] = titulos.find("p",{"class":"volanta"}).text.replace("\r","").replace("\n","").replace("\t","")
    except:
      n['volanta'] = None
    try:
      n['titulo'] = titulos.find("h1",{"id":"title"}).text.replace("\r","").replace("\n","").replace("\t","")
    except:
      n['titulo'] = None
    try:
      n['resumen'] = titulos.find("div",{"class":"bajada"}).text.replace("\r","").replace("\n","").replace("\t","")
    except:
      n['resumen'] = None

    if imagen_ppal != None:
      try:
        n['epigrafe'] = imagen_ppal.find("p").text.replace("\r","").replace("\n","").replace("\t","")
      except:
        n['epigrafe'] = None

    try:
      parrafos = body.findAll("p")
    except:
      return {'link_noticia': link_noticia, 'fecha': None, 'volanta': None, 'titulo': None, 'resumen': None, 'epigrafe': None, 'titulos_parrafos': None, 'parrafos': None,}
    titulos_parrafos = body.findAll("h2")

    text_parrafo = ""
    for item in parrafos:
      if "COMENTARIOS" in item.text:
        break
      else:
        text_parrafo += "||" + item.text

    text_titulos_parrafos = ""
    for item in titulos_parrafos:
       text_titulos_parrafos += "||" + item.text

    n['titulos_parrafos'] = text_titulos_parrafos.strip().replace("TEMAS QUE APARECEN EN ESTA NOTA","").replace("\r","").replace("\n","").replace("\t","")
    n['parrafos'] = text_parrafo.strip().replace("TEMAS QUE APARECEN EN ESTA NOTA","").replace("\r","").replace("\n","").replace("\t","")

    return n

def scrap_clarin(pag,with_webdriver = False, webdriver_dir = ""):
  l = []

  noticias = obtiene_noticias_clarin(pag, with_webdriver, webdriver_dir)
  for noticia in noticias:
    scrap_not = lee_noticia_clarin(noticia)
    if scrap_not != None:
      l.append(scrap_not)
  return pd.DataFrame(l)


"""## La Nación"""

def obtiene_noticias_lanacion(pag, usar_webdriver = False, webdriver_dir = ""):
  
  if usar_webdriver == False:
    resp = requests.get(pag)
    soup = BeautifulSoup(resp.content, features = "html")
  else:
    browser = webdriver.Chrome(webdriver_dir)
    browser.get(pag)
    box_scrap.display_buttom()
    soup = BeautifulSoup(browser.page_source,"html.parser",multi_valued_attributes=None)

    
  div_noticias = soup.findAll("article", {"class": "nota"})
  noticias = []
  for div in div_noticias:
    noticias.append("https://www.lanacion.com.ar/" + div.find("h2").find("a").get("href"))
  return noticias

def lee_noticia_lanacion(link_noticia):
    print(link_noticia)
    n = {}
    n['link_noticia'] = link_noticia
    resp = requests.get(link_noticia)
    
    # si la respuesta del servidor es mala, sale de la ejecución de esta noticia.
    if resp.ok == False:
      return None

    soup = BeautifulSoup(resp.content, features = "html")

    # obtiene distintas partes de la noticia:
    try:
      articulo_completo = soup.find("article", {"id": "nota"})
    except:
      return {'link_noticia': link_noticia}
    # info = articulo_completo.find("div", {"class": "article-info"})
    try:
      titulos = articulo_completo.find("section", {"class": "encabezado"})
    except:
      return {'link_noticia': link_noticia}
    try:
      body = articulo_completo.find("section", {"id": "cuerpo"})
    except:
      return {'link_noticia': link_noticia}

    # obtiene atributos

    try:
      raw_fecha = body.find("section", {"class":"fecha"}).text.replace("\r","").replace("\n","").replace("\t","")
      n['fecha'] = timpestamp_lanacion(mes_esp_to_eng(raw_fecha))
    except:
      n['fecha'] = None

    try:
      n['seccion'] = titulos.find("strong", {"class": "categoria"}).text.replace("\r","").replace("\n","").replace("\t","")
    except:
      n['seccion'] = None
    try:
      n['titulo'] = titulos.find("h1",{"class":"titulo"}).text.replace("\r","").replace("\n","").replace("\t","")
    except:
      n['titulo'] = None
    try:
      n['resumen'] = titulos.find("div",{"class":"article-summary"}).text
    except:
      n['resumen'] = None
    try:
      n['epigrafe'] = body.find("epigrafe").text.replace("\r","").replace("\n","").replace("\t","")
    except:
      n['epigrafe'] = None


    parrafos = body.findAll("p")
    titulos_parrafos = body.findAll("h2")

    text_parrafo = ""
    for item in parrafos:
      text_parrafo += "||" + item.text

    text_titulos_parrafos = ""
    for item in titulos_parrafos:
      text_titulos_parrafos += "||" + item.text

    n['titulos_parrafos'] = text_titulos_parrafos.strip().replace("\r","").replace("\n","").replace("\t","")
    n['parrafos'] = text_parrafo.strip().replace("Conforme a los criterios de","").replace("\r","").replace("\n","").replace("\t","")

    return n

def scrap_lanacion(pags, with_webdriver = False, webdriver_dir = ""):
  l = []

  for pag in pags:
    noticias = obtiene_noticias_lanacion(pag, with_webdriver, webdriver_dir)
    for noticia in noticias:
      scrap_not = lee_noticia_lanacion(noticia)
      if scrap_not != None:
        l.append(scrap_not)
  df = pd.DataFrame(l)
  df['volanta'] = None
  return df

"""## El cronista"""

def obtiene_noticias_elcronista(pag, usar_webdriver = False, webdriver_dir = ""):
  
  if usar_webdriver == False:
    resp = requests.get(pag)
    soup = BeautifulSoup(resp.content, features = "html")
  else:
    browser = webdriver.Chrome(webdriver_dir)
    browser.get(pag)
    box_scrap.display_buttom()
    soup = BeautifulSoup(browser.page_source,"html.parser",multi_valued_attributes=None)

    
  div_noticias = soup.findAll("article")
  noticias = []
  for div in div_noticias:
    link = div.find("a").get("href")
    if link.startswith("http",0,4):
      pass
    else:
      noticias.append("https://www.cronista.com/" + link)
  return noticias

def lee_noticia_elcronista(link_noticia):
    print(link_noticia)
    n = {}
    n['link_noticia'] = link_noticia
    resp = requests.get(link_noticia)
    
    # si la respuesta del servidor es mala, sale de la ejecución de esta noticia.
    if resp.ok == False:
      return None

    soup = BeautifulSoup(resp.content, features = "html")

    # obtiene distintas partes de la noticia:

    try:
      titulos = soup.find("header", {"class": "header-nota"})
    except:
      return {'link_noticia': link_noticia}

    try:
      body = soup.find("div", {"class": "article-container"})
      # return body
    except:
      return {'link_noticia': link_noticia}

    # obtiene atributos

    try:
      raw_fecha = soup.find("meta", {"itemprop":"datePublished"}).attrs['content']
      n['fecha'] = timpestamp_elcronista(raw_fecha)
    except:
      n['fecha'] = None

    try:
      n['seccion'] = titulos.find("span",{"class":"entry-label-2"}).text.replace("\r","").replace("\n","").replace("\t","")
    except:
      n['seccion'] = None
    try:
      n['titulo'] = titulos.find("h1",{"class":"titulo-nota"}).text.replace("\r","").replace("\n","").replace("\t","")
    except:
      n['titulo'] = None
    try:
      n['resumen'] = titulos.find("p",{"class":"bajada"}).text.replace("\r","").replace("\n","").replace("\t","")
    except:
      n['resumen'] = None
    try:
      n['epigrafe'] = soup.find("figcaption",{"class": "note-cape"}).text.replace("\r","").replace("\n","").replace("\t","")
    except:
      n['epigrafe'] = None

    
    parrafos = body.findAll("p")

    contenido_listas = body.findAll("li")

    text_parrafo = ""
    for item in parrafos:
      text_parrafo += "||" + item.text

    for item in contenido_listas:
      text_parrafo += "||" + item.text

    titulos_parrafos = body.findAll("h4")
    text_titulos_parrafos = ""
    for item in titulos_parrafos:
      text_titulos_parrafos += "||" + item.text
    
    titulos_parrafos = body.findAll("h3")
    for item in titulos_parrafos:
      text_titulos_parrafos += "||" + item.text

    n['titulos_parrafos'] = text_titulos_parrafos.strip().replace("\r","").replace("\n","").replace("\t","")
    n['parrafos'] = text_parrafo.strip().replace("\r","").replace("\n","").replace("\t","")

    return n

def scrap_elcronista(pag, with_webdriver = False, webdriver_dir = ""):
  l = []

  noticias = obtiene_noticias_elcronista(pag, with_webdriver, webdriver_dir)
  for noticia in noticias:
    scrap_not = lee_noticia_elcronista(noticia)
    if scrap_not != None:
      l.append(scrap_not)
  df = pd.DataFrame(l)
  df['volanta'] = None
  return df


"""## Infobae"""

def obtiene_noticias_infobae(pag, usar_webdriver = False, webdriver_dir = ""):
  
  if usar_webdriver == False:
    resp = requests.get(pag)
    soup = BeautifulSoup(resp.content, features = "html")
  else:
    browser = webdriver.Chrome(webdriver_dir)
    browser.get(pag)
    box_scrap.display_buttom()
    soup = BeautifulSoup(browser.page_source,"html.parser",multi_valued_attributes=None)

  noticias = []

  # obtiene noticia principal
  main_new = soup.find("div",{"class": "main_new col mobile-12 tablet-8"})
  noticias.append("https://www.infobae.com/" + main_new.find("a").get("href"))

  # obtiene noticias secundarias
  second_new_panel = soup.find("div",{"class": "secondary_news col mobile-12 tablet-4"})
  second_news = second_new_panel.findAll("a")
  for link in second_news:
    noticias.append("https://www.infobae.com/" + link.get("href"))

  # obtiene noticias del body superior
  body_sup = soup.find("div",{"class": "body | page-container margin_auto grid width_full"})
  body_noticias = body_sup.findAll("div",{"class": "width_full padding_vertical_10px "})
  for div in body_noticias:
    links = div.findAll("a")
    for link in links:
      noticias.append("https://www.infobae.com/" + link.get("href"))
  
  # obtiene las noticias del body inferior:
  body_inf = soup.find("div",{"class": "minibody | inner_minibody grid col widescreen-8 desktop-8 tablet-8"})
  body_noticias_encabezados = body_inf.findAll("h2")
  for noticia in body_noticias_encabezados:
    noticias.append("https://www.infobae.com/" + noticia.find("a").get("href"))

  # Obtiene las noticias del panel dolar
  dolar_panel = soup.find("div",{"id": "0fzxTVPrCWPY5MC"})
  dolar_noticias = dolar_panel.findAll("div",{"class": "hidden_desktop"})
  for noticia in dolar_noticias:
    noticias.append("https://www.infobae.com/" + noticia.find("a").get("href"))
  return list(set(noticias))

def lee_noticia_infobae(link_noticia):
    print(link_noticia)
    n = {}
    n['link_noticia'] = link_noticia
    resp = requests.get(link_noticia)

    # si la respuesta del servidor es mala, sale de la ejecución de esta noticia.
    if resp.ok == False:
      return None

    soup = BeautifulSoup(resp.content, features = "html")

    # obtiene distintas partes de la noticia:

    try:
      titulos = soup.find("div", {"class": "article-header font_tertiary"})
    except:
      return {'link_noticia': link_noticia}

    # obtiene atributos

    try:
      raw_fecha = soup.find("div", {"class":"datetime | display_inline byline_datetime"}).text.replace("\r","").replace("\n","").replace("\t","")
      n['fecha'] = timpestamp_infobae(mes_esp_to_eng_upper(raw_fecha))
    except:
      n['fecha'] = None

    try:
      n['seccion'] = titulos.find("div").text.replace("\r","").replace("\n","").replace("\t","")
    except:
      n['seccion'] = None
    try:
      n['titulo'] = titulos.find("h1").text.replace("\r","").replace("\n","").replace("\t","")
    except:
      n['titulo'] = None
    try:
      n['resumen'] = titulos.find("h2").text.replace("\r","").replace("\n","").replace("\t","")
    except:
      n['resumen'] = None
    try:
      n['epigrafe'] = soup.find("figcaption",{"class": "padding_vertical_15px padding_horizontal_15px"}).text.replace("\r","").replace("\n","").replace("\t","")
    except:
      n['epigrafe'] = None


    parrafos = soup.findAll("p",{"class": "font_tertiary paragraph"})

    text_parrafo = ""
    for item in parrafos:
      text_parrafo += "||" + item.text

    # for item in contenido_listas:
    #   text_parrafo += ". " + item.text

    # titulos_parrafos = body.findAll("h4")
    # text_titulos_parrafos = ""
    # for item in titulos_parrafos:
    #   text_titulos_parrafos += " " + item.text
    
    # titulos_parrafos = body.findAll("h3")
    # for item in titulos_parrafos:
    #   text_titulos_parrafos += " " + item.text

    # n['titulos_parrafos'] = text_titulos_parrafos.strip().replace("\r","").replace("\n","").replace("\t","")
    n['parrafos'] = text_parrafo.strip().replace("\r","").replace("\n","").replace("\t","")

    return n

def scrap_infobae(pag, with_webdriver = False, webdriver_dir = ""):
  l = []

  noticias = obtiene_noticias_infobae(pag, with_webdriver, webdriver_dir)
  for noticia in noticias:
    scrap_not = lee_noticia_infobae(noticia)
    if scrap_not != None:
      l.append(scrap_not)
  df = pd.DataFrame(l)
  df['volanta'] = None
  df['titulos_parrafos'] = None
  return df


"""# Ambito"""

def obtiene_noticias_ambito(pag_base, pag):
  noticias = []
  resp = requests.get(pag_base + str(pag))
  soup = BeautifulSoup(resp.content, features = "html")
  
  # obtenemos la noticia principal
  if pag == 0:
    main_new = soup.find("article",{"class": "highlighted-note-overprinted-title"})
    noticias.append(main_new.find("a").get("href"))
    main_new_2 = soup.find("article",{"class": "simple-note-square-image"})
    noticias.append(main_new_2.find("a").get("href"))
    four_news_panel = soup.find("section",{"class": "simple-list-square-image four-column"})
    four_news = four_news_panel.findAll("h2")
    for noticia in four_news:
      noticias.append(noticia.find("a").get("href"))

  # obtenemos las noticias listadas abajo
  div_noticias_list = soup.find("div", {"class": "list-articles"})
  div_noticias = div_noticias_list.findAll("h2", {"class": "title"})
  for div in div_noticias:
    noticias.append(div.find("a").get("href"))
  return noticias

def lee_noticia_ambito(link_noticia):
    print(link_noticia)
    n = {}
    n['link_noticia'] = link_noticia
    resp = requests.get(link_noticia)

    # si la respuesta del servidor es mala, sale de la ejecución de esta noticia.
    if resp.ok == False:
      return None

    soup = BeautifulSoup(resp.content, features = "html")

    # obtiene distintas partes de la noticia:

    try:
      titulos = soup.find("div", {"class": "detail-header"})
    except:
      return {'link_noticia': link_noticia, 'fecha': None, 'epigrafe': None, 'seccion': None, 'titulo': None, 'resumen': None, 'parrafos': None}

    # try:
    #   body = soup.find("div", {"class": "article-container"})
    # except:
    #   return {'link_noticia': link_noticia, 'fecha': = None, 'epigrafe': = None, 'seccion': = None, 'titulo': = None, 'resumen': = None, 'parrafos': None}

    # obtiene atributos
    try:
      raw_fecha = titulos.find("time").attrs['datetime'].replace("\r","").replace("\n","").replace("\t","")
      n['fecha'] = timpestamp_ambito(mes_esp_to_eng(raw_fecha))

    except:
      n['fecha'] = None

    try:
      n['seccion'] = titulos.find("span", {"class": "article-theme"}).text.replace("\r","").replace("\n","").replace("\t","")
    except:
      n['seccion'] = None
    try:
      n['titulo'] = titulos.find("h1",{"class": "title"}).text.replace("\r","").replace("\n","").replace("\t","")
    except:
      n['titulo'] = None
    try:
      n['resumen'] = titulos.find("h2", {"class": "excerpt"}).text.replace("\r","").replace("\n","").replace("\t","")
    except:
      n['resumen'] = None
    try:
      n['epigrafe'] = soup.find("span",{"class": "epiinterior"}).text.replace("\r","").replace("\n","").replace("\t","")
    except:
      n['epigrafe'] = None

    text_parrafo = ""
    # Obtenemos info del primer parrafo:
    try:
      primer_parrafo = soup.find("section", {"class": "detail-body first-paragraph"})
      primeros_parrafos = primer_parrafo.findAll("p")
      for item in primeros_parrafos:
        text_parrafo += "||" + item.text
    except:
      pass

    # Obtenemos info de segundo parrafo
    try:
      segundo_parrafo = soup.find("section", {"class": "detail-body second-paragraph"})
      segundos_parrafos = segundo_parrafo.findAll("p")
      for item in segundos_parrafos:
        text_parrafo += "||" + item.text
    except:
      pass

    # Obtenemos los parrafos del body
    try:
      body = soup.find(lambda tag: tag.name == 'section' and 
                                   tag.get('class') == ['detail-body'])
      body_parrafos = body.findAll("p")
      for item in body_parrafos:
        text_parrafo += "||" + item.text
    except:
      pass

    # for item in contenido_listas:
    #   text_parrafo += ". " + item.text

    # titulos_parrafos = body.findAll("h4")
    # text_titulos_parrafos = ""
    # for item in titulos_parrafos:
    #   text_titulos_parrafos += " " + item.text
    
    # titulos_parrafos = body.findAll("h3")
    # for item in titulos_parrafos:
    #   text_titulos_parrafos += " " + item.text

    # n['titulos_parrafos'] = text_titulos_parrafos.strip().replace("\r","").replace("\n","").replace("\t","")
    n['parrafos'] = text_parrafo.strip().replace("El contenido al que quiere acceder es exclusivo para suscriptores.","").replace("\r","").replace("\n","").replace("\t","")

    return n

def scrap_ambito(desde_pag, hasta_pag):
  l = []
  for pag in range(desde_pag,hasta_pag + 1):
    noticias = obtiene_noticias_ambito("https://www.ambito.com/contenidos/economia.html/", pag)
    for noticia in noticias:
      scrap_not = lee_noticia_ambito(noticia)
      if scrap_not != None:
        l.append(scrap_not)
  df = pd.DataFrame(l)
  df['volanta'] = None
  df['titulos_parrafos'] = None
  return df


"""# Crea el dataframe final"""

today = str(datetime.date(datetime.now()))

df_clarin = scrap_clarin("https://www.clarin.com/economia/", True, DRIVER_DIR)
df_clarin.to_csv(OUTPUT_DIR + "\\" + "df_clarin_" +today.replace("-","") + ".csv",encoding="utf-8")

# https://www.pagina12.com.ar/secciones/economia
# df_pagina12 = scrap_pag12(0,8)
# df_pagina12.to_csv(OUTPUT_DIR + "\\" + "df_pagina12_" +today.replace("-","") + ".csv",encoding="utf-8")

# pags = ["https://www.lanacion.com.ar/economia/negocios",
#         # "https://www.lanacion.com.ar/dolar-hoy",
#         "https://www.lanacion.com.ar/economia/campo",
#         "https://www.lanacion.com.ar/tema/emprendedores-tid53673"
#         ]
# df_lanacion = scrap_lanacion(pags, True, DRIVER_DIR)
# df_lanacion.to_csv(OUTPUT_DIR + "\\" + "df_lanacion_" +today.replace("-","") + ".csv",encoding="utf-8")

# df_clarin = scrap_clarin("https://www.clarin.com/economia/", True, DRIVER_DIR)
# df_clarin.to_csv(OUTPUT_DIR + "\\" + "df_clarin_" +today.replace("-","") + ".csv",encoding="utf-8")

# df_elcronista = scrap_elcronista("https://www.cronista.com/seccion/economia_politica/", True, DRIVER_DIR)
# df_elcronista.to_csv(OUTPUT_DIR + "\\" + "df_elcronista_" +today.replace("-","") + ".csv",encoding="utf-8")

# df_infobae = scrap_infobae("https://www.infobae.com/economia/", True, DRIVER_DIR)
# df_infobae.to_csv(OUTPUT_DIR + "\\" + "df_infobae_" +today.replace("-","") + ".csv",encoding="utf-8")

# # https://www.ambito.com/contenidos/economia.html
# df_ambito = scrap_ambito(0,4)
# df_ambito.to_csv(OUTPUT_DIR + "\\" + "df_ambito_" +today.replace("-","") + ".csv",encoding="utf-8")

# # Lee los df guardados en CSV y 



# df_final = pd.concat([df_pagina12, 
#                       df_clarin,
#                       df_lanacion,
#                       df_elcronista,
#                       df_infobae,
#                       df_ambito])

# df_final.to_csv(OUTPUT_DIR + "\\" + "scrap_diarios_" +today.replace("-","") + ".csv",encoding="utf-8")